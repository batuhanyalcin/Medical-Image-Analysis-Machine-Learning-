{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP 448/548 – Medical Image Analysis\n",
        "##            Homework #3\n",
        "\n",
        "Eren Berke Demirbaş\n",
        "64627\n",
        "Batuhan Yalçın\n",
        "64274\n",
        "\n",
        "25.05.2022\n",
        "\n",
        "[link text](https://colab.research.google.com/drive/1Jjk1ZASbFzSlDXXThrAROoL-ofsXtpq7?usp=sharing)"
      ],
      "metadata": {
        "id": "fHJIyFUdetEx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ1U0F2Nc_p6"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr4kZxEHdX00",
        "outputId": "792289a4-f9ac-418c-e11b-b75105045fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory = '/content/drive/MyDrive/COMP448_HW3/main_dir/'\n",
        "os.chdir(data_directory)"
      ],
      "metadata": {
        "id": "NfQ_dgTzd2M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        " 'train': transforms.Compose([\n",
        " # put the input to Tensor format in order to use torch\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(), \n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        " ]),\n",
        " 'valid': transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "   ]),                        \n",
        " 'test': transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                     \n",
        "   ])\n",
        "}\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_directory, x),\n",
        " data_transforms[x]) for x in ['train', 'valid', 'test']}"
      ],
      "metadata": {
        "id": "L-zljAOqe8c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_weights_of_balanced_classes(images, K):\n",
        "    counter = list(0 for i in range(0, K))\n",
        "    balanced_weights = {}\n",
        "    for type in ['train', 'valid', 'test']:\n",
        "        for element in images[type]:\n",
        "            counter[element[1]] = counter[element[1]] + 1\n",
        "        class_weights = list(0.0 for i in range(0,K))\n",
        "        for c in range(K):\n",
        "            class_weights[c] = sum(counter) / counter[c]\n",
        "        #print(len(images[type]))\n",
        "        weight = list(0 for i in range(0, len(images[type])))\n",
        "        for index, value in enumerate(images[type]):\n",
        "            weight[index] = class_weights[value[1]]\n",
        "        balanced_weights[type] = weight\n",
        "    return balanced_weights  \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "2WVLMugzK9JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_weights = determine_weights_of_balanced_classes(image_datasets, 3)\n",
        "print(balanced_weights)\n",
        "sampler = {x: torch.utils.data.sampler.WeightedRandomSampler(balanced_weights[x], len(balanced_weights[x])) for x in ['train', 'valid', 'test']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],batch_size = 4, shuffle = True, num_workers = 4) for x in ['train', 'valid', 'test']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFLcQU8ZhLof",
        "outputId": "42dd00ac-a40f-40a2-af5f-da3530e71a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': [3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 3.26, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 2.0632911392405062, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823, 4.794117647058823], 'valid': [3.1, 3.1, 3.1, 3.1, 3.1, 3.1, 3.1, 3.1, 3.1, 3.1, 2.1136363636363638, 2.1136363636363638, 2.1136363636363638, 2.1136363636363638, 2.1136363636363638, 2.1136363636363638, 2.1136363636363638, 2.1136363636363638, 2.1136363636363638, 4.894736842105263, 4.894736842105263, 4.894736842105263, 4.894736842105263], 'test': [3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 3.0555555555555554, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 2.2758620689655173, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286, 4.285714285714286]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer,\n",
        " scheduler, num_epochs):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_no_corrects = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        # Set the model to the training mode for updating the weights using\n",
        "        # the first portion of training images\n",
        "        model.train()\n",
        "        for inputs, labels in dataloaders['train']: # iterate over data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(True):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        # Set the model to the evaluation mode for selecting the best network\n",
        "        # based on the number of correctly classified validation images\n",
        "        model.eval()\n",
        "        no_corrects = 0\n",
        "        for inputs, labels in dataloaders['valid']:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                no_corrects += torch.sum(preds == labels.data)\n",
        "            if no_corrects > best_no_corrects:\n",
        "                best_no_corrects = no_corrects\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            scheduler.step()\n",
        "    # Load the weights of the best network\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "XVQtvI26hhTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_conv = models.alexnet(pretrained = True)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "K_GVdJSci3lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = model_conv.classifier[6].in_features # applies a linear transformation to the incoming data\n",
        "print(features)\n",
        "model_conv.classifier[6] = nn.Linear(features, 3)\n",
        "model_conv = model_conv.to(device) # use the GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnYgdrtWjZCx",
        "outputId": "b8982814-88e2-46f9-8c4b-4d48d57abfaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() #Computes the entropy loss between input and target\n",
        "optimizer_conv = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=1, gamma=0.1)\n",
        "model_conv.eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yUzrnACnvwA",
        "outputId": "af20d5fe-6816-42bd-a1eb-1c09282b2b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.eval of AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_conv = train_model(model_conv, criterion, optimizer_conv,exp_lr_scheduler, num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWycIa3ZpF0X",
        "outputId": "6c56af4f-f59d-4da9-b14b-904a3fa8e140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.functional import normalize\n",
        "#Pytorch confusion matrix\n",
        "confusion_matrix = torch.zeros(3, 3)\n",
        "with torch.no_grad():\n",
        "  for case in ['train', 'valid', 'test']:\n",
        "      for i, (inputs, classes) in enumerate(dataloaders[case]):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = model_conv(inputs)\n",
        "        tests, preds = torch.max(outputs, 1)\n",
        "        for labels, predict in zip(classes.view(-1), preds.view(-1)):\n",
        "          confusion_matrix[labels.long(), predict.long()] += 1\n",
        "\n",
        "      print(\"Confusion Matrix for \",case ,\"is:\\n\",confusion_matrix)\n",
        "      #Vectorize Confusion matrix and get class accuracies\n",
        "      Class_Accuracy=confusion_matrix.diagonal()/(confusion_matrix.sum(0) - confusion_matrix.diagonal() + confusion_matrix.sum(1) )\n",
        "      print(\"Class Accuracies are\",Class_Accuracy)\n",
        "      #Overall accuracy\n",
        "      Overall_accuracy = confusion_matrix.diagonal().sum()/confusion_matrix.sum()\n",
        "      print(\"Overall_accuracy is :\",Overall_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXJvif42sDnJ",
        "outputId": "3623a62d-f01d-469a-d17b-3194085f4984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for  train is:\n",
            " tensor([[49.,  1.,  0.],\n",
            "        [ 8., 70.,  1.],\n",
            "        [ 9.,  3., 22.]])\n",
            "Class Accuracies are tensor([0.7313, 0.8434, 0.6286])\n",
            "Overall_accuracy is : tensor(0.8650)\n",
            "Confusion Matrix for  valid is:\n",
            " tensor([[59.,  1.,  0.],\n",
            "        [10., 77.,  1.],\n",
            "        [10.,  4., 24.]])\n",
            "Class Accuracies are tensor([0.7375, 0.8280, 0.6154])\n",
            "Overall_accuracy is : tensor(0.8602)\n",
            "Confusion Matrix for  test is:\n",
            " tensor([[104.,   4.,   0.],\n",
            "        [ 13., 127.,   5.],\n",
            "        [ 21.,   7.,  49.]])\n",
            "Class Accuracies are tensor([0.7324, 0.8141, 0.5976])\n",
            "Overall_accuracy is : tensor(0.8485)\n"
          ]
        }
      ]
    }
  ]
}